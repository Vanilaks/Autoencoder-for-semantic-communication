{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vanilaks/Autoencoder-for-semantic-communication/blob/main/Autoencoder_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the code for combined autoencoder and transformer, which is trained together (unlike the paper where the autoencoder is pretrained).\n",
        "Autoencoder compresses d_model = 512 length vector to N = 64 length vector. The evaluation is done with no noise."
      ],
      "metadata": {
        "id": "JqlMXduR6rqD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Eb79B_Dqdr0c",
        "outputId": "134fd2fb-b960-482f-871c-bdc8f84fea0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext==0.15.2\n",
            "  Downloading torchtext-0.15.2-cp311-cp311-manylinux1_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.2) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.2) (2.32.3)\n",
            "Collecting torch==2.0.1 (from torchtext==0.15.2)\n",
            "  Downloading torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.2) (2.0.2)\n",
            "Collecting torchdata==0.6.1 (from torchtext==0.15.2)\n",
            "  Downloading torchdata-0.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->torchtext==0.15.2) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->torchtext==0.15.2) (4.13.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->torchtext==0.15.2) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->torchtext==0.15.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->torchtext==0.15.2) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1->torchtext==0.15.2)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1->torchtext==0.15.2)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1->torchtext==0.15.2)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1->torchtext==0.15.2)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1->torchtext==0.15.2)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1->torchtext==0.15.2)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1->torchtext==0.15.2)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1->torchtext==0.15.2)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1->torchtext==0.15.2)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1->torchtext==0.15.2)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1->torchtext==0.15.2)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.0.0 (from torch==2.0.1->torchtext==0.15.2)\n",
            "  Downloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.6.1->torchtext==0.15.2) (2.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchtext==0.15.2) (75.2.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchtext==0.15.2) (0.45.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1->torchtext==0.15.2) (3.31.6)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.1->torchtext==0.15.2)\n",
            "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.15.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.15.2) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.15.2) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.1->torchtext==0.15.2) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.1->torchtext==0.15.2) (1.3.0)\n",
            "Downloading torchtext-0.15.2-cp311-cp311-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchdata-0.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, torchdata, torchtext\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed lit-18.1.8 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.1 torchdata-0.6.1 torchtext-0.15.2 triton-2.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torchtext==0.15.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SEPTt5X5ds4a",
        "outputId": "12bef0cf-8b5b-426a-d8b6-5b027f109b1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip3 install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XUatxQ0dwNL",
        "outputId": "5fd217ea-07e1-4cb1-9698-8266cd9f1020"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy<2\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.4\n",
            "    Uninstalling numpy-2.2.4:\n",
            "      Successfully uninstalled numpy-2.2.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        }
      ],
      "source": [
        "!pip3 install --force-reinstall \"numpy<2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "M7mpSWmijykK",
        "outputId": "36c4e6b2-7433-406b-e475-f752e19ae2e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.11/dist-packages (2.5.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (3.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (1.26.4)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install sacrebleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qoz74sabgzBx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, eps:float=10**-6) -> None:\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.alpha = nn.Parameter(torch.ones(features)) # alpha is a learnable parameter\n",
        "        self.bias = nn.Parameter(torch.zeros(features)) # bias is a learnable parameter\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, hidden_size)\n",
        "         # Keep the dimension for broadcasting\n",
        "        mean = x.mean(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
        "        # Keep the dimension for broadcasting\n",
        "        std = x.std(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
        "        # eps is to prevent dividing by zero or when std is very small\n",
        "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n",
        "\n",
        "class FeedForwardBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
        "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n",
        "\n",
        "class InputEmbeddings(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch, seq_len) --> (batch, seq_len, d_model)\n",
        "        # Multiply by sqrt(d_model) to scale the embeddings according to the paper\n",
        "        return self.embedding(x) * math.sqrt(self.d_model)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.seq_len = seq_len\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # Create a matrix of shape (seq_len, d_model)\n",
        "        pe = torch.zeros(seq_len, d_model)\n",
        "        # Create a vector of shape (seq_len)\n",
        "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
        "        # Create a vector of shape (d_model)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
        "        # Apply sine to even indices\n",
        "        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n",
        "        # Apply cosine to odd indices\n",
        "        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n",
        "        # Add a batch dimension to the positional encoding\n",
        "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
        "        # Register the positional encoding as a buffer\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n",
        "        return self.dropout(x)\n",
        "\n",
        "class ResidualConnection(nn.Module):\n",
        "\n",
        "        def __init__(self, features: int, dropout: float) -> None:\n",
        "            super().__init__()\n",
        "            self.dropout = nn.Dropout(dropout)\n",
        "            self.norm = LayerNormalization(features)\n",
        "\n",
        "        def forward(self, x, sublayer):\n",
        "            return x + self.dropout(sublayer(self.norm(x)))\n",
        "\n",
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model # Embedding vector size\n",
        "        self.h = h # Number of heads\n",
        "        # Make sure d_model is divisible by h\n",
        "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
        "\n",
        "        self.d_k = d_model // h # Dimension of vector seen by each head\n",
        "        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n",
        "        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\n",
        "        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\n",
        "        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
        "        d_k = query.shape[-1]\n",
        "        # Just apply the formula from the paper\n",
        "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
        "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "        if mask is not None:\n",
        "            # Write a very low value (indicating -inf) to the positions where mask == 0\n",
        "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
        "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
        "        if dropout is not None:\n",
        "            attention_scores = dropout(attention_scores)\n",
        "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
        "        # return attention scores which can be used for visualization\n",
        "        return (attention_scores @ value), attention_scores\n",
        "\n",
        "    def forward(self, q, k, v, mask):\n",
        "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
        "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Calculate attention\n",
        "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
        "\n",
        "        # Combine all the heads together\n",
        "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
        "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
        "\n",
        "        # Multiply by Wo\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        return self.w_o(x)\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
        "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization(features)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.cross_attention_block = cross_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
        "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
        "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization(features)\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "class ProjectionLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, vocab_size) -> None:\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x) -> None:\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
        "        return self.proj(x)\n",
        "\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_dim, M, N):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, M),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(M, 2 * N),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(2 * N, M),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(M, input_dim)\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "    def decode(self, x):\n",
        "        return self.decoder(x)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer, autoencoder: Autoencoder) -> None:\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.src_pos = src_pos\n",
        "        self.tgt_pos = tgt_pos\n",
        "        self.projection_layer = projection_layer\n",
        "        self.autoencoder = autoencoder  # Autoencoder is integrated here\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        # (batch, seq_len, d_model)\n",
        "        src = self.src_embed(src)\n",
        "        src = self.src_pos(src)\n",
        "        encoder_output = self.encoder(src, src_mask)\n",
        "\n",
        "        # Reshape encoder output to match the input shape of the autoencoder\n",
        "        # Assuming the encoder output has shape (batch, seq_len, d_model)\n",
        "        # You need to flatten this into (batch * seq_len, d_model) to match the input of autoencoder\n",
        "\n",
        "        batch_size, seq_len, d_model = encoder_output.shape\n",
        "        encoder_output_flat = encoder_output.view(batch_size * seq_len, d_model)  # (batch * seq_len, d_model)\n",
        "\n",
        "        # Pass the flattened encoder output through the autoencoder\n",
        "        autoencoded_output = self.autoencoder.encode(encoder_output_flat)  # (batch * seq_len, d_model)\n",
        "\n",
        "        return autoencoded_output\n",
        "\n",
        "    def decode(self, autoencoded_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
        "        # (batch, seq_len, d_model)\n",
        "        tgt = self.tgt_embed(tgt)\n",
        "        tgt = self.tgt_pos(tgt)\n",
        "\n",
        "        # Reshape it back to (batch, seq_len, d_model)\n",
        "        batch_size, seq_len, d_model = tgt.shape\n",
        "        encoder_output_reconstr = self.autoencoder.decode(autoencoded_output)\n",
        "        encoder_output = encoder_output_reconstr.view(batch_size, seq_len, d_model)  # (batch, seq_len, d_model)\n",
        "\n",
        "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "    def project(self, x):\n",
        "        # (batch, seq_len, vocab_size)\n",
        "        return self.projection_layer(x)\n",
        "\n",
        "\n",
        "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int=512, N: int=6, h: int=8, dropout: float=0.1, d_ff: int=2048) -> Transformer:\n",
        "    # Create the embedding layers\n",
        "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
        "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
        "\n",
        "    # Create the positional encoding layers\n",
        "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
        "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
        "\n",
        "    # Create the encoder blocks\n",
        "    encoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
        "        encoder_blocks.append(encoder_block)\n",
        "\n",
        "    # Create the decoder blocks\n",
        "    decoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
        "        decoder_blocks.append(decoder_block)\n",
        "\n",
        "    # Create the encoder and decoder\n",
        "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
        "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n",
        "\n",
        "    # Create the projection layer\n",
        "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
        "\n",
        "    # Load the pretrained autoencoder model\n",
        "    autoencoder = Autoencoder(input_dim=d_model, M=256, N=64)\n",
        "    # autoencoder.load_state_dict(torch.load(\"autoencoder_awgn.pth\"))\n",
        "    # autoencoder.eval()  # Set the autoencoder to evaluation mode\n",
        "\n",
        "    # Create the transformer\n",
        "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer, autoencoder)\n",
        "\n",
        "    # Initialize the parameters\n",
        "    for p in transformer.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "\n",
        "    return transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SC-SdC2deF3"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Train.py with special characters removed from the dataset\n",
        "'''\n",
        "\n",
        "from model import build_transformer\n",
        "from dataset import BilingualDataset, causal_mask, clean_text\n",
        "from config import get_config, get_weights_file_path, latest_weights_file_path\n",
        "\n",
        "import torchtext.datasets as datasets\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Huggingface datasets and tokenizers\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "# import torchmetrics\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQVeZPmhdgM0",
        "outputId": "be289852-6947-4881-fcd4-0226c30ea7ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Max length of source sentence: 204\n",
            "Max length of target sentence: 204\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 00: 100%|██████████| 421/421 [01:04<00:00,  6.49it/s, loss=5.7686]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: the cook threw a fryingpan after her as she went out but it just missed her\n",
            "TARGET: the cook threw a fryingpan after her as she went out but it just missed her\n",
            "PREDICTED: she a a a\n",
            "BLEU Score: 0.0048\n",
            "CHRF Score: 3.0783\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: i see said the queen who had meanwhile been examining the roses\n",
            "TARGET: i see said the queen who had meanwhile been examining the roses\n",
            "PREDICTED: i said the\n",
            "BLEU Score: 0.0132\n",
            "CHRF Score: 11.7025\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Loss: 5.3514, BLEU Score: 0.0189, CHRF Score: 10.0628\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 01: 100%|██████████| 421/421 [01:03<00:00,  6.58it/s, loss=5.0971]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: pray what is the reason of that\n",
            "TARGET: pray what is the reason of that\n",
            "PREDICTED: what you know the king\n",
            "BLEU Score: 0.0428\n",
            "CHRF Score: 16.0237\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: consider your verdict he said to the jury in a low trembling voice\n",
            "TARGET: consider your verdict he said to the jury in a low trembling voice\n",
            "PREDICTED: i dont dont said the mock turtle\n",
            "BLEU Score: 0.0167\n",
            "CHRF Score: 14.3277\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Loss: 4.9145, BLEU Score: 0.0305, CHRF Score: 14.2032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 02: 100%|██████████| 421/421 [01:03<00:00,  6.59it/s, loss=4.2140]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: on this the white rabbit blew three blasts on the trumpet and then unrolled the parchment scroll and read as follows\n",
            "TARGET: on this the white rabbit blew three blasts on the trumpet and then unrolled the parchment scroll and read as follows\n",
            "PREDICTED: on the queen on the gryphon and the gryphon and the gryphon and the gryphon\n",
            "BLEU Score: 0.0256\n",
            "CHRF Score: 17.2253\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: who is it directed to said one of the jurymen\n",
            "TARGET: who is it directed to said one of the jurymen\n",
            "PREDICTED: it said the mock turtle to the gryphon\n",
            "BLEU Score: 0.0306\n",
            "CHRF Score: 19.3469\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Loss: 4.5723, BLEU Score: 0.0396, CHRF Score: 18.1373\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 03: 100%|██████████| 421/421 [01:04<00:00,  6.57it/s, loss=3.4513]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: you shant be beheaded said alice and she put them into a large flowerpot that stood near\n",
            "TARGET: you shant be beheaded said alice and she put them into a large flowerpot that stood near\n",
            "PREDICTED: you be said alice and she that\n",
            "BLEU Score: 0.1133\n",
            "CHRF Score: 25.9497\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: how queer it seems alice said to herself to be going messages for a rabbit\n",
            "TARGET: how queer it seems alice said to herself to be going messages for a rabbit\n",
            "PREDICTED: how it would be said alice to herself for a little\n",
            "BLEU Score: 0.0480\n",
            "CHRF Score: 29.6382\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Loss: 4.1072, BLEU Score: 0.0773, CHRF Score: 24.7615\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 04: 100%|██████████| 421/421 [01:04<00:00,  6.53it/s, loss=3.7420]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: what will become of me\n",
            "TARGET: what will become of me\n",
            "PREDICTED: what are you are me\n",
            "BLEU Score: 0.0639\n",
            "CHRF Score: 16.0080\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: i told you butter wouldnt suit the works he added looking angrily at the march hare\n",
            "TARGET: i told you butter wouldnt suit the works he added looking angrily at the march hare\n",
            "PREDICTED: i must you were say added the he said the gryphon\n",
            "BLEU Score: 0.0187\n",
            "CHRF Score: 16.2420\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Loss: 3.6261, BLEU Score: 0.1358, CHRF Score: 31.7090\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 05: 100%|██████████| 421/421 [01:03<00:00,  6.60it/s, loss=3.5859]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: as she said this she looked up and there was the cat again sitting on a branch of a tree\n",
            "TARGET: as she said this she looked up and there was the cat again sitting on a branch of a tree\n",
            "PREDICTED: as she said this she looked down and was the cat was too much a a of a\n",
            "BLEU Score: 0.3416\n",
            "CHRF Score: 47.0772\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: its always six oclock now\n",
            "TARGET: its always six oclock now\n",
            "PREDICTED: its no use now\n",
            "BLEU Score: 0.0744\n",
            "CHRF Score: 12.6837\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Loss: 3.2425, BLEU Score: 0.2030, CHRF Score: 39.5127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 06: 100%|██████████| 421/421 [01:03<00:00,  6.58it/s, loss=2.6655]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: she said it to the knave of hearts who only bowed and smiled in reply\n",
            "TARGET: she said it to the knave of hearts who only bowed and smiled in reply\n",
            "PREDICTED: she said to it of the knave who who only and in another moment\n",
            "BLEU Score: 0.0567\n",
            "CHRF Score: 38.6091\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: its always six oclock now\n",
            "TARGET: its always six oclock now\n",
            "PREDICTED: its sure im afraid now\n",
            "BLEU Score: 0.0639\n",
            "CHRF Score: 14.2771\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Loss: 2.9204, BLEU Score: 0.2611, CHRF Score: 44.6021\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 07: 100%|██████████| 421/421 [01:04<00:00,  6.55it/s, loss=2.8927]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: how puzzling all these changes are\n",
            "TARGET: how puzzling all these changes are\n",
            "PREDICTED: how all all these are are\n",
            "BLEU Score: 0.1027\n",
            "CHRF Score: 31.2780\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: i didnt the march hare interrupted in a great hurry\n",
            "TARGET: i didnt the march hare interrupted in a great hurry\n",
            "PREDICTED: i didnt march the march hare in a great hurry\n",
            "BLEU Score: 0.4234\n",
            "CHRF Score: 65.2872\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Loss: 2.6835, BLEU Score: 0.3319, CHRF Score: 50.6358\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 08: 100%|██████████| 421/421 [01:04<00:00,  6.57it/s, loss=2.7538]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: swim after them screamed the gryphon\n",
            "TARGET: swim after them screamed the gryphon\n",
            "PREDICTED: after them them the gryphon\n",
            "BLEU Score: 0.1316\n",
            "CHRF Score: 52.5814\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: the caterpillar and alice looked at each other for some time in silence at last the caterpillar took the hookah out of its mouth and addressed her in a languid sleepy voice\n",
            "TARGET: the caterpillar and alice looked at each other for some time in silence at last the caterpillar took the hookah out of its mouth and addressed her in a languid sleepy voice\n",
            "PREDICTED: the caterpillar and alice looked at once for some time in silence in at last the caterpillar took the caterpillar took its mouth and the in a voice\n",
            "BLEU Score: 0.4557\n",
            "CHRF Score: 64.3933\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Loss: 2.4821, BLEU Score: 0.3761, CHRF Score: 54.8064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 09: 100%|██████████| 421/421 [01:04<00:00,  6.58it/s, loss=1.5716]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: everybody looked at alice\n",
            "TARGET: everybody looked at alice\n",
            "PREDICTED: everybody looked at alice\n",
            "BLEU Score: 1.0000\n",
            "CHRF Score: 100.0000\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: the executioners argument was that you couldnt cut off a head unless there was a body to cut it off from that he had never had to do such a thing before and he wasnt going to begin at his time of life\n",
            "TARGET: the executioners argument was that you couldnt cut off a head unless there was a body to cut it off from that he had never had to do such a thing before and he wasnt going to begin at his time of life\n",
            "PREDICTED: the argument was that you couldnt answer off a head there was a large cat to explain it had off that he had never to do that he was going to do at his eye and and he was at his head\n",
            "BLEU Score: 0.2337\n",
            "CHRF Score: 47.8594\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Loss: 2.3384, BLEU Score: 0.4262, CHRF Score: 60.7296\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 421/421 [01:03<00:00,  6.61it/s, loss=1.9075]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: if they had any sense theyd take the roof off\n",
            "TARGET: if they had any sense theyd take the roof off\n",
            "PREDICTED: if they had any rate take the sky off\n",
            "BLEU Score: 0.3301\n",
            "CHRF Score: 49.0229\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: swim after them screamed the gryphon\n",
            "TARGET: swim after them screamed the gryphon\n",
            "PREDICTED: after them them screamed the gryphon\n",
            "BLEU Score: 0.5774\n",
            "CHRF Score: 85.8108\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Loss: 2.1645, BLEU Score: 0.4676, CHRF Score: 63.9841\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11: 100%|██████████| 421/421 [01:03<00:00,  6.58it/s, loss=1.3685]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: where are you\n",
            "TARGET: where are you\n",
            "PREDICTED: where are you\n",
            "BLEU Score: 0.5623\n",
            "CHRF Score: 100.0000\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: they were indeed a queerlooking party that assembled on the bankthe birds with draggled feathers the animals with their fur clinging close to them and all dripping wet cross and uncomfortable\n",
            "TARGET: they were indeed a queerlooking party that assembled on the bankthe birds with draggled feathers the animals with their fur clinging close to them and all dripping wet cross and uncomfortable\n",
            "PREDICTED: they were indeed a party that on the birds with with the birds with their slates and and all all all\n",
            "BLEU Score: 0.1276\n",
            "CHRF Score: 33.9466\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Loss: 2.1055, BLEU Score: 0.4906, CHRF Score: 66.0926\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12: 100%|██████████| 421/421 [01:03<00:00,  6.58it/s, loss=1.3149]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: youre a serpent and theres no use denying it\n",
            "TARGET: youre a serpent and theres no use denying it\n",
            "PREDICTED: youre a serpent and theres no use it\n",
            "BLEU Score: 0.7673\n",
            "CHRF Score: 77.7180\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: she got up and went to the table to measure herself by it and found that as nearly as she could guess she was now about two feet high and was going on shrinking rapidly she soon found out that the cause of this was the fan she was holding and she dropped it hastily just in time to avoid shrinking away altogether\n",
            "TARGET: she got up and went to the table to measure herself by it and found that as nearly as she could guess she was now about two feet high and was going on shrinking rapidly she soon found out that the cause of this was the fan she was holding and she dropped it hastily just in time to avoid shrinking away altogether\n",
            "PREDICTED: she got up and went to the table to herself and found that it that as she could see she was now about two about two about two and going on going on found out of the door out of the door that it was just of the door she was sneezing and the door she was away\n",
            "BLEU Score: 0.2704\n",
            "CHRF Score: 45.4715\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Loss: 2.0401, BLEU Score: 0.5245, CHRF Score: 68.4880\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13: 100%|██████████| 421/421 [01:04<00:00,  6.54it/s, loss=1.4829]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: nobody moved\n",
            "TARGET: nobody moved\n",
            "PREDICTED: nobody moved\n",
            "BLEU Score: 0.3162\n",
            "CHRF Score: 100.0000\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: just then her head struck against the roof of the hall in fact she was now more than nine feet high and she at once took up the little golden key and hurried off to the garden door\n",
            "TARGET: just then her head struck against the roof of the hall in fact she was now more than nine feet high and she at once took up the little golden key and hurried off to the garden door\n",
            "PREDICTED: just then her head finished the roof of the hall in fact she was now was more than feet high she high she took up and took the little golden key and the door\n",
            "BLEU Score: 0.4478\n",
            "CHRF Score: 60.6708\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Loss: 1.9812, BLEU Score: 0.5287, CHRF Score: 69.0091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14: 100%|██████████| 421/421 [01:04<00:00,  6.53it/s, loss=1.7022]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: sounds of more broken glass\n",
            "TARGET: sounds of more broken glass\n",
            "PREDICTED: stuff of more glass glass\n",
            "BLEU Score: 0.1257\n",
            "CHRF Score: 30.8195\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: yes we went to school in the sea though you maynt believe it\n",
            "TARGET: yes we went to school in the sea though you maynt believe it\n",
            "PREDICTED: yes we went to school in the sea you believe it\n",
            "BLEU Score: 0.6335\n",
            "CHRF Score: 69.3567\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Loss: 1.9292, BLEU Score: 0.5424, CHRF Score: 69.0447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15: 100%|██████████| 421/421 [01:04<00:00,  6.52it/s, loss=1.3189]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: i dare say you never even spoke to time\n",
            "TARGET: i dare say you never even spoke to time\n",
            "PREDICTED: i dare say you never even to time\n",
            "BLEU Score: 0.6753\n",
            "CHRF Score: 75.8853\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: they all can said the duchess and most of em do\n",
            "TARGET: they all can said the duchess and most of em do\n",
            "PREDICTED: they all can said the duchess and most chorus of tea\n",
            "BLEU Score: 0.6989\n",
            "CHRF Score: 82.4413\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Loss: 1.9689, BLEU Score: 0.5446, CHRF Score: 70.2681\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16: 100%|██████████| 421/421 [01:05<00:00,  6.46it/s, loss=1.6136]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: but there seemed to be no chance of this so she began looking at everything about her to pass away the time\n",
            "TARGET: but there seemed to be no chance of this so she began looking at everything about her to pass away the time\n",
            "PREDICTED: but there seemed to be no chance of this so she began looking at everything her everything to the time\n",
            "BLEU Score: 0.7115\n",
            "CHRF Score: 78.6957\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: what sort of people live about here\n",
            "TARGET: what sort of people live about here\n",
            "PREDICTED: what sort of people live about here\n",
            "BLEU Score: 1.0000\n",
            "CHRF Score: 100.0000\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Loss: 1.8793, BLEU Score: 0.5837, CHRF Score: 72.1770\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17: 100%|██████████| 421/421 [01:04<00:00,  6.54it/s, loss=1.2481]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: just then her head struck against the roof of the hall in fact she was now more than nine feet high and she at once took up the little golden key and hurried off to the garden door\n",
            "TARGET: just then her head struck against the roof of the hall in fact she was now more than nine feet high and she at once took up the little golden key and hurried off to the garden door\n",
            "PREDICTED: just then her head carrying against the roof of the hall in fact she was now more than saying high and she found she took up at the little golden key and hurried off\n",
            "BLEU Score: 0.5825\n",
            "CHRF Score: 72.6763\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: nobody moved\n",
            "TARGET: nobody moved\n",
            "PREDICTED: nobody moved moved\n",
            "BLEU Score: 0.2403\n",
            "CHRF Score: 89.2257\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Loss: 1.8686, BLEU Score: 0.5786, CHRF Score: 72.3350\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18: 100%|██████████| 421/421 [01:04<00:00,  6.52it/s, loss=1.2530]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: i told you butter wouldnt suit the works he added looking angrily at the march hare\n",
            "TARGET: i told you butter wouldnt suit the works he added looking angrily at the march hare\n",
            "PREDICTED: i told you wouldnt wouldnt wouldnt the he added looking at the march hare\n",
            "BLEU Score: 0.2981\n",
            "CHRF Score: 57.2701\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: thinking again the duchess asked with another dig of her sharp little chin\n",
            "TARGET: thinking again the duchess asked with another dig of her sharp little chin\n",
            "PREDICTED: thinking again the duchess asked another of her flamingo of little chin\n",
            "BLEU Score: 0.3839\n",
            "CHRF Score: 68.4907\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Loss: 1.7878, BLEU Score: 0.6081, CHRF Score: 74.6599\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19: 100%|██████████| 421/421 [01:04<00:00,  6.53it/s, loss=1.2638]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: if they had any sense theyd take the roof off\n",
            "TARGET: if they had any sense theyd take the roof off\n",
            "PREDICTED: if they had any rate take less the roof off\n",
            "BLEU Score: 0.3928\n",
            "CHRF Score: 58.6056\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: alice had been looking over his shoulder with some curiosity\n",
            "TARGET: alice had been looking over his shoulder with some curiosity\n",
            "PREDICTED: alice had been over his notebook somebody with some curiosity\n",
            "BLEU Score: 0.1996\n",
            "CHRF Score: 61.8309\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Loss: 1.7829, BLEU Score: 0.5709, CHRF Score: 71.8806\n",
            "Training complete. Metrics saved to training_metrics.pkl\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import pickle  # To save losses, BLEU scores, and CHRF scores\n",
        "import sys\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from sacrebleu import corpus_chrf\n",
        "\n",
        "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
        "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
        "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
        "    encoder_output = model.encode(source, source_mask)\n",
        "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
        "    while True:\n",
        "        if decoder_input.size(1) == max_len:\n",
        "            break\n",
        "        decoder_mask = None\n",
        "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
        "        prob = model.project(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        decoder_input = torch.cat([decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1)\n",
        "        if next_word == eos_idx:\n",
        "            break\n",
        "    return decoder_input.squeeze(0)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def calculate_snr(clean_signal, noise_signal):\n",
        "    \"\"\"\n",
        "    Calculate the Signal-to-Noise Ratio (SNR).\n",
        "    - clean_signal: The ground truth signal (target).\n",
        "    - noise_signal: The predicted signal (model output).\n",
        "    \"\"\"\n",
        "    # Reshape the signals to have the same number of dimensions\n",
        "    clean_signal = clean_signal.flatten()\n",
        "    noise_signal = noise_signal.flatten()\n",
        "\n",
        "    signal_power = np.sum(clean_signal ** 2)\n",
        "    noise_power = np.sum((clean_signal - noise_signal) ** 2)\n",
        "\n",
        "    # Avoid division by zero\n",
        "    if noise_power == 0:\n",
        "        return float('inf')\n",
        "\n",
        "    snr = 10 * np.log10(signal_power / noise_power)\n",
        "    return snr\n",
        "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_step, writer, num_examples=2):\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    count = 0\n",
        "    bleu_scores = []\n",
        "    chrf_scores = []\n",
        "    snr_scores = []  # List to store SNR scores\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n",
        "    smooth_fn = SmoothingFunction().method1\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in validation_ds:\n",
        "            count += 1\n",
        "            encoder_input = batch[\"encoder_input\"].to(device)\n",
        "            encoder_mask = batch[\"encoder_mask\"].to(device)\n",
        "            label = batch[\"label\"].to(device)\n",
        "            decoder_input = batch[\"decoder_input\"].to(device)\n",
        "            decoder_mask = batch[\"decoder_mask\"].to(device)\n",
        "\n",
        "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
        "            proj_output = model.project(model.decode(model.encode(encoder_input, encoder_mask), encoder_mask, decoder_input, decoder_mask))\n",
        "            val_loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "            predicted_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n",
        "            target_text = batch[\"tgt_text\"][0]\n",
        "\n",
        "            # BLEU calculation\n",
        "            reference = [target_text.split()]\n",
        "            hypothesis = predicted_text.split()\n",
        "            bleu = sentence_bleu(reference, hypothesis, smoothing_function=smooth_fn)\n",
        "            bleu_scores.append(bleu)\n",
        "\n",
        "            # CHRF calculation\n",
        "            chrf = corpus_chrf([predicted_text], [[target_text]]).score\n",
        "            chrf_scores.append(chrf)\n",
        "\n",
        "            # # SNR calculation (Assuming 'encoder_input' is the clean signal and 'model_out' is the noisy signal)\n",
        "            # snr = calculate_snr(encoder_input.cpu().numpy(), model_out.cpu().numpy())\n",
        "            # snr_scores.append(snr)\n",
        "\n",
        "            if count <= num_examples:\n",
        "                source_text = batch[\"src_text\"][0]\n",
        "                print_msg('-' * 80)\n",
        "                print_msg(f\"SOURCE: {source_text}\")\n",
        "                print_msg(f\"TARGET: {target_text}\")\n",
        "                print_msg(f\"PREDICTED: {predicted_text}\")\n",
        "                print_msg(f\"BLEU Score: {bleu:.4f}\")\n",
        "                print_msg(f\"CHRF Score: {chrf:.4f}\")\n",
        "                # print_msg(f\"SNR Score: {snr:.4f}\")\n",
        "\n",
        "        print_msg('-' * 80)\n",
        "\n",
        "    avg_val_loss = total_val_loss / count if count > 0 else 0\n",
        "    avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0\n",
        "    avg_chrf = sum(chrf_scores) / len(chrf_scores) if chrf_scores else 0\n",
        "\n",
        "    print(f\"Validation Loss: {avg_val_loss:.4f}, BLEU Score: {avg_bleu:.4f}, CHRF Score: {avg_chrf:.4f}\")\n",
        "    writer.add_scalar('Validation Loss', avg_val_loss, global_step)\n",
        "    writer.add_scalar('Validation BLEU', avg_bleu, global_step)\n",
        "    writer.add_scalar('Validation CHRF', avg_chrf, global_step)\n",
        "    writer.flush()\n",
        "\n",
        "    # Return SNR scores along with other metrics\n",
        "    return avg_val_loss, avg_bleu, avg_chrf, snr_scores\n",
        "\n",
        "def get_all_sentences(ds, lang):\n",
        "    for item in ds:\n",
        "        yield clean_text(item['translation'][lang])\n",
        "\n",
        "def get_or_build_tokenizer(config, ds, lang):\n",
        "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
        "    if not Path.exists(tokenizer_path):\n",
        "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
        "        tokenizer.pre_tokenizer = Whitespace()\n",
        "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
        "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
        "        tokenizer.save(str(tokenizer_path))\n",
        "    else:\n",
        "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
        "    return tokenizer\n",
        "\n",
        "def get_ds(config):\n",
        "    # It only has the train split, so we divide it overselves\n",
        "    ds_raw = load_dataset(f\"{config['datasource']}\", f\"{config['ds_lang_src']}-{config['ds_lang_tgt']}\", split='train')\n",
        "\n",
        "    # Build tokenizers\n",
        "    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config['lang_src'])\n",
        "    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config['lang_tgt'])\n",
        "\n",
        "    # Find the maximum length of each sentence in the source and target sentence\n",
        "    max_len_src = 0\n",
        "    max_len_tgt = 0\n",
        "\n",
        "    for item in ds_raw:\n",
        "        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n",
        "        tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids\n",
        "        max_len_src = max(max_len_src, len(src_ids))\n",
        "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
        "\n",
        "    print(f'Max length of source sentence: {max_len_src}')\n",
        "    print(f'Max length of target sentence: {max_len_tgt}')\n",
        "\n",
        "    # Keep 90% for training, 10% for validation\n",
        "    train_ds_size = int(0.9 * len(ds_raw))\n",
        "    val_ds_size = len(ds_raw) - train_ds_size\n",
        "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
        "\n",
        "    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "\n",
        "    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n",
        "    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
        "\n",
        "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\n",
        "\n",
        "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
        "    model = build_transformer(vocab_src_len, vocab_tgt_len, config[\"seq_len\"], config['seq_len'], d_model=config['d_model'])\n",
        "    return model\n",
        "\n",
        "def train_model(config):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
        "    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n",
        "    writer = SummaryWriter(config['experiment_name'])\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    bleu_scores = []\n",
        "    chrf_scores = []\n",
        "    # snr_scores_all = []  # List to store SNR scores over all epochs\n",
        "    global_step = 0\n",
        "\n",
        "    for epoch in range(config['num_epochs']):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        count = 0\n",
        "\n",
        "        batch_iterator = tqdm(train_dataloader, desc=f\"Epoch {epoch:02d}\")\n",
        "        for batch in batch_iterator:\n",
        "            encoder_input = batch['encoder_input'].to(device)\n",
        "            decoder_input = batch['decoder_input'].to(device)\n",
        "            encoder_mask = batch['encoder_mask'].to(device)\n",
        "            decoder_mask = batch['decoder_mask'].to(device)\n",
        "            label = batch['label'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            encoder_output = model.encode(encoder_input, encoder_mask)\n",
        "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
        "            proj_output = model.project(decoder_output)\n",
        "\n",
        "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "            count += 1\n",
        "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
        "            writer.add_scalar('Train Loss', loss.item(), global_step)\n",
        "            writer.flush()\n",
        "            global_step += 1\n",
        "\n",
        "        avg_train_loss = total_train_loss / count if count > 0 else 0\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Run validation and collect SNR scores\n",
        "        avg_val_loss, avg_bleu, avg_chrf, snr_scores = run_validation(\n",
        "            model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device,\n",
        "            lambda msg: batch_iterator.write(msg), global_step, writer\n",
        "        )\n",
        "        val_losses.append(avg_val_loss)\n",
        "        bleu_scores.append(avg_bleu)\n",
        "        chrf_scores.append(avg_chrf)\n",
        "        # snr_scores_all.extend(snr_scores)  # Add SNR scores from this epoch\n",
        "\n",
        "        # Save training metrics (including SNR scores)\n",
        "        with open('training_metrics.pkl', 'wb') as f:\n",
        "            pickle.dump({\n",
        "                'train_losses': train_losses,\n",
        "                'val_losses': val_losses,\n",
        "                'bleu_scores': bleu_scores,\n",
        "                'chrf_scores': chrf_scores,\n",
        "                # 'snr_scores': snr_scores_all  # Save all SNR scores\n",
        "            }, f)\n",
        "\n",
        "    print(\"Training complete. Metrics saved to training_metrics.pkl\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "    config = get_config()\n",
        "    config['num_epochs'] = 20\n",
        "    train_model(config)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}